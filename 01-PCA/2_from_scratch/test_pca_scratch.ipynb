{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Our Custom PCA Implementation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that we've built PCA from scratch, let's test it thoroughly to make sure it works correctly!\n",
    "\n",
    "### What You'll Learn\n",
    "1. How to use our custom PCA class\n",
    "2. Verify implementation with manual calculations\n",
    "3. Test all methods: fit, transform, inverse_transform\n",
    "4. Visualize results\n",
    "5. Test edge cases\n",
    "\n",
    "### Testing Strategy\n",
    "- Use simple 2D data for easy verification\n",
    "- Compare with manual calculations\n",
    "- Test different n_components settings\n",
    "- Visualize transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our custom PCA implementation\n",
    "from pca_implementation import PCA, plot_explained_variance, biplot\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"âœ“ Custom PCA module loaded from: {Path('pca_implementation.py').absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Basic Functionality with 2D Data\n",
    "\n",
    "Let's start with the same simple 2D dataset we used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "np.random.seed(42)\n",
    "X = np.array([\n",
    "    [2.5, 2.4],\n",
    "    [0.5, 0.7],\n",
    "    [2.2, 2.9],\n",
    "    [1.9, 2.2],\n",
    "    [3.1, 3.0],\n",
    "    [2.3, 2.7],\n",
    "    [2.0, 1.6],\n",
    "    [1.0, 1.1]\n",
    "])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(X)\n",
    "print(f\"\\nShape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"PCA Model:\")\n",
    "print(pca)\n",
    "print(\"\\nComponents (Principal axes):\")\n",
    "print(pca.components_)\n",
    "print(\"\\nMean:\")\n",
    "print(pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Compare with Manual Calculation\n",
    "\n",
    "Let's verify our implementation matches manual PCA calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual PCA calculation\n",
    "print(\"Manual PCA Calculation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Center data\n",
    "mean_manual = X.mean(axis=0)\n",
    "X_centered_manual = X - mean_manual\n",
    "print(f\"Mean: {mean_manual}\")\n",
    "\n",
    "# Step 2: Covariance matrix\n",
    "cov_manual = np.cov(X_centered_manual.T)\n",
    "print(f\"\\nCovariance matrix:\\n{cov_manual}\")\n",
    "\n",
    "# Step 3: Eigenvalues and eigenvectors\n",
    "eigenvalues_manual, eigenvectors_manual = np.linalg.eig(cov_manual)\n",
    "idx = eigenvalues_manual.argsort()[::-1]\n",
    "eigenvalues_manual = eigenvalues_manual[idx]\n",
    "eigenvectors_manual = eigenvectors_manual[:, idx]\n",
    "\n",
    "print(f\"\\nEigenvalues: {eigenvalues_manual}\")\n",
    "print(f\"Eigenvectors:\\n{eigenvectors_manual}\")\n",
    "\n",
    "# Step 4: Compare with our implementation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparison: Manual vs Our Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nMean match:\", np.allclose(mean_manual, pca.mean_))\n",
    "print(\"Explained variance match:\", np.allclose(eigenvalues_manual, pca.explained_variance_))\n",
    "print(\"Components match:\", np.allclose(eigenvectors_manual.T, pca.components_))\n",
    "\n",
    "if np.allclose(eigenvalues_manual, pca.explained_variance_):\n",
    "    print(\"\\nâœ“ SUCCESS: Our implementation matches manual calculation!\")\n",
    "else:\n",
    "    print(\"\\nâœ— ERROR: Results don't match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Transform and Inverse Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transform\n",
    "X_transformed = pca.transform(X)\n",
    "print(\"Transformed Data (PCA space):\")\n",
    "print(X_transformed)\n",
    "print(f\"Shape: {X_transformed.shape}\")\n",
    "\n",
    "# Manual transformation\n",
    "X_transformed_manual = X_centered_manual.dot(eigenvectors_manual)\n",
    "print(\"\\nManual Transformation:\")\n",
    "print(X_transformed_manual)\n",
    "\n",
    "print(\"\\nTransform match:\", np.allclose(X_transformed, X_transformed_manual))\n",
    "\n",
    "# Test inverse transform\n",
    "X_reconstructed = pca.inverse_transform(X_transformed)\n",
    "print(\"\\nReconstructed Data (back to original space):\")\n",
    "print(X_reconstructed)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error = np.mean((X - X_reconstructed) ** 2)\n",
    "print(f\"\\nReconstruction MSE: {reconstruction_error:.10f}\")\n",
    "print(\"Perfect reconstruction:\", reconstruction_error < 1e-10)\n",
    "\n",
    "if reconstruction_error < 1e-10:\n",
    "    print(\"\\nâœ“ SUCCESS: Perfect reconstruction (all components kept)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Dimensionality Reduction\n",
    "\n",
    "Now let's test keeping only 1 component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 1D\n",
    "pca_1d = PCA(n_components=1)\n",
    "X_1d = pca_1d.fit_transform(X)\n",
    "\n",
    "print(\"1D Reduction Results:\")\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Reduced shape: {X_1d.shape}\")\n",
    "print(f\"\\nVariance explained: {pca_1d.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"\\n1D Data:\\n{X_1d.ravel()}\")\n",
    "\n",
    "# Reconstruct from 1D\n",
    "X_reconstructed_1d = pca_1d.inverse_transform(X_1d)\n",
    "reconstruction_error_1d = np.mean((X - X_reconstructed_1d) ** 2)\n",
    "\n",
    "print(f\"\\nReconstruction MSE (1 component): {reconstruction_error_1d:.6f}\")\n",
    "print(f\"Information loss: {(1 - pca_1d.explained_variance_ratio_[0]):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 1D reduction and reconstruction\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "ax1.scatter(X[:, 0], X[:, 1], s=150, alpha=0.7, color='blue',\n",
    "           edgecolors='k', linewidths=2, label='Original', zorder=3)\n",
    "ax1.scatter(X_reconstructed_1d[:, 0], X_reconstructed_1d[:, 1], s=150,\n",
    "           alpha=0.7, color='red', marker='s', edgecolors='k', linewidths=2,\n",
    "           label='Reconstructed (1 PC)', zorder=3)\n",
    "\n",
    "# Draw reconstruction error lines\n",
    "for i in range(len(X)):\n",
    "    ax1.plot([X[i, 0], X_reconstructed_1d[i, 0]],\n",
    "            [X[i, 1], X_reconstructed_1d[i, 1]],\n",
    "            'k--', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax1.set_xlabel('Feature 1', fontsize=12)\n",
    "ax1.set_ylabel('Feature 2', fontsize=12)\n",
    "ax1.set_title('Reconstruction from 1 Component', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 1D representation\n",
    "ax2.scatter(X_1d, np.zeros_like(X_1d), s=150, alpha=0.7,\n",
    "           c=range(len(X_1d)), cmap='viridis', edgecolors='k', linewidths=2)\n",
    "for i, val in enumerate(X_1d.ravel(), 1):\n",
    "    ax2.annotate(f'S{i}', (val, 0), xytext=(0, 10),\n",
    "                textcoords='offset points', ha='center')\n",
    "ax2.axhline(0, color='gray', linewidth=2)\n",
    "ax2.set_xlabel('PC1 Value', fontsize=12)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_title(f'1D Representation ({pca_1d.explained_variance_ratio_[0]:.1%} variance)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Variance Threshold\n",
    "\n",
    "Test automatic component selection based on variance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with variance threshold\n",
    "print(\"Testing variance threshold selection:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for threshold in [0.90, 0.95, 0.99]:\n",
    "    pca_threshold = PCA(n_components=threshold)\n",
    "    pca_threshold.fit(X)\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold:.0%}\")\n",
    "    print(f\"  Components selected: {pca_threshold.n_components_}\")\n",
    "    print(f\"  Actual variance: {pca_threshold.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Utility Functions\n",
    "\n",
    "Test the visualization utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plot_explained_variance\n",
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test biplot\n",
    "feature_names = ['Nitrogen (ppm)', 'Phosphorus (ppm)']\n",
    "biplot(pca, X, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Higher Dimensional Data\n",
    "\n",
    "Test with more realistic multi-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5D correlated data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "\n",
    "# Create correlated features\n",
    "base = np.random.randn(n_samples, 2)\n",
    "X_5d = np.column_stack([\n",
    "    base[:, 0] + np.random.randn(n_samples) * 0.1,\n",
    "    base[:, 0] * 0.8 + np.random.randn(n_samples) * 0.2,\n",
    "    base[:, 1] + np.random.randn(n_samples) * 0.1,\n",
    "    base[:, 1] * 0.7 + np.random.randn(n_samples) * 0.3,\n",
    "    np.random.randn(n_samples) * 0.5  # Independent feature\n",
    "])\n",
    "\n",
    "print(f\"5D Data shape: {X_5d.shape}\")\n",
    "print(f\"\\nFirst 5 samples:\\n{X_5d[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on 5D data\n",
    "pca_5d = PCA(n_components=5)\n",
    "X_5d_transformed = pca_5d.fit_transform(X_5d)\n",
    "\n",
    "print(\"5D PCA Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nExplained variance by component:\")\n",
    "for i, var in enumerate(pca_5d.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i}: {var:.2%}\")\n",
    "\n",
    "cumsum = np.cumsum(pca_5d.explained_variance_ratio_)\n",
    "print(f\"\\nCumulative variance:\")\n",
    "for i, var in enumerate(cumsum, 1):\n",
    "    print(f\"  First {i} PCs: {var:.2%}\")\n",
    "\n",
    "# Find components needed for 95% variance\n",
    "n_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "print(f\"\\nâœ“ {n_95} components needed for 95% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance for 5D data\n",
    "plot_explained_variance(pca_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 2 PCs\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_5d_transformed[:, 0], X_5d_transformed[:, 1],\n",
    "           alpha=0.6, s=50, edgecolors='k', linewidths=0.5,\n",
    "           c=range(n_samples), cmap='viridis')\n",
    "plt.xlabel(f'PC1 ({pca_5d.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_5d.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "plt.title('5D Data Projected onto First 2 Principal Components', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Sample index')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Additional Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test get_covariance\n",
    "cov_estimated = pca.get_covariance()\n",
    "cov_actual = np.cov(X.T)\n",
    "\n",
    "print(\"Covariance Matrix Estimation:\")\n",
    "print(\"\\nActual covariance:\")\n",
    "print(cov_actual)\n",
    "print(\"\\nEstimated covariance:\")\n",
    "print(cov_estimated)\n",
    "print(\"\\nClose match:\", np.allclose(cov_actual, cov_estimated, rtol=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score method\n",
    "score = pca.score(X)\n",
    "print(f\"Score (negative reconstruction error): {score:.6f}\")\n",
    "print(\"\\nNote: Higher score is better (closer to 0 = better reconstruction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with n_components = None\n",
    "pca_all = PCA(n_components=None)\n",
    "pca_all.fit(X)\n",
    "print(f\"n_components=None: Kept {pca_all.n_components_} components (all)\")\n",
    "\n",
    "# Test with n_components > n_features\n",
    "pca_large = PCA(n_components=10)\n",
    "pca_large.fit(X)\n",
    "print(f\"\\nn_components=10 (> n_features): Kept {pca_large.n_components_} components (max possible)\")\n",
    "\n",
    "# Test error handling\n",
    "try:\n",
    "    pca_uninit = PCA()\n",
    "    pca_uninit.transform(X)  # Should raise error\n",
    "except ValueError as e:\n",
    "    print(f\"\\nâœ“ Proper error handling: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" \" * 20 + \"TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tests = [\n",
    "    \"âœ“ Basic functionality (fit, transform)\",\n",
    "    \"âœ“ Manual calculation verification\",\n",
    "    \"âœ“ Transform and inverse transform\",\n",
    "    \"âœ“ Dimensionality reduction (2D â†’ 1D)\",\n",
    "    \"âœ“ Variance threshold selection\",\n",
    "    \"âœ“ Utility functions (plotting)\",\n",
    "    \"âœ“ Higher dimensional data (5D)\",\n",
    "    \"âœ“ Additional methods (covariance, score)\",\n",
    "    \"âœ“ Edge cases and error handling\"\n",
    "]\n",
    "\n",
    "for test in tests:\n",
    "    print(test)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸŽ‰ ALL TESTS PASSED! Our PCA implementation is working correctly!\")\n",
    "print(\"\\nReady to compare with sklearn in the next notebook.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Verified\n",
    "\n",
    "1. **Correctness**: Our implementation matches manual calculations\n",
    "2. **Completeness**: All methods work as expected\n",
    "3. **Robustness**: Handles edge cases properly\n",
    "4. **Flexibility**: Works with different dimensions and parameters\n",
    "\n",
    "### Understanding Gained\n",
    "\n",
    "- PCA is fundamentally about eigendecomposition\n",
    "- The choice of n_components involves trade-offs\n",
    "- Reconstruction error quantifies information loss\n",
    "- Visualization helps understand what PCA does\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now we'll:\n",
    "1. Learn to use sklearn's optimized PCA\n",
    "2. Compare our implementation with sklearn\n",
    "3. Apply PCA to real agricultural data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Excellent work!** You've thoroughly tested a PCA implementation from scratch.\n",
    "\n",
    "Continue to: `../3_with_sklearn/sklearn_pca_basics.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
